{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implementation - PyTorch V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 50000\n",
    "MIN_REPLAY_SIZE = 1000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY = 10000\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "LEARNING_RATE = 5e-4\n",
    "MAX_STEPS = 200000\n",
    "\n",
    "# Regularization Coefficient\n",
    "REGULARIZATION_COEFFICIENT = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_matrix = []\n",
    "observations_t = None\n",
    "new_observations_t = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default loss function - mean squared error\n",
    "\n",
    "def default_loss_mse(y_true, y_pred):\n",
    "    \n",
    "    loss = torch.mean(torch.square(y_true-y_pred))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function - implements explicit DR3 regularizer\n",
    "\n",
    "# add dot product between each state action and subsequent oneâ€™s feature vector to loss\n",
    "def dr3(y_true, y_pred):\n",
    "    \n",
    "    global observations_t, new_observations_t\n",
    "    \n",
    "    loss = torch.mean(torch.square(y_true-y_pred))\n",
    "    \n",
    "    if observations_t != None and new_observations_t != None:\n",
    "        \n",
    "        for i in range(len(observations_t)):          \n",
    "            curr_state_feature_vector = online_net.get_phi(observations_t[i]).cpu().detach().numpy()\n",
    "            next_state_feature_vector = online_net.get_phi(new_observations_t[i]).cpu().detach().numpy()\n",
    "            loss += REGULARIZATION_COEFFICIENT * np.dot(curr_state_feature_vector, next_state_feature_vector)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function - random dot product from phi matrix\n",
    "\n",
    "# randomly sample two vectors from the phi matrix and add dot product of those vectors to loss\n",
    "def random_dot(y_true, y_pred):\n",
    "\n",
    "    global phi_matrix\n",
    "    \n",
    "    loss = torch.mean(torch.square(y_true-y_pred))\n",
    "    \n",
    "    # Explicit Regularization\n",
    "    if ((phi_matrix is not None) and (len(phi_matrix) > 1)):\n",
    "        \n",
    "        v1 = phi_matrix[random.randrange(len(phi_matrix))]\n",
    "        v2 = phi_matrix[random.randrange(len(phi_matrix))]\n",
    "        \n",
    "        loss += REGULARIZATION_COEFFICIENT * np.dot(np.array(v1), np.array(v2))\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function - implements regulizer based on min/max singular values in phi matrix\n",
    "\n",
    "# add difference between max entry in phi matrix ** 2 and min entry in phi matrix ** 2 to loss\n",
    "def phi_penalty(y_true, y_pred):\n",
    "    \n",
    "    global phi_matrix\n",
    "    \n",
    "    loss = torch.mean(torch.square(y_true-y_pred))\n",
    "    \n",
    "    # Explicit Regularization\n",
    "    if ((phi_matrix is not None) and (len(phi_matrix) > 0)):\n",
    "        minimum = min([min(value) for value in phi_matrix])\n",
    "        maximum = max([max(value) for value in phi_matrix])\n",
    "        loss += REGULARIZATION_COEFFICIENT * (maximum**2 - minimum**2)\n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network class\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super().__init__()        \n",
    "        in_features = int(np.prod(env.observation_space.shape))     \n",
    "        # Neural Network\n",
    "        self.layer1 = nn.Linear(in_features, 24)\n",
    "        self.layer2 = nn.ReLU()\n",
    "        self.layer3 = nn.ReLU()\n",
    "        self.layer4 = nn.Linear(24, env.action_space.n)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer4(self.layer3(self.layer2(self.layer1(x))))\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self(obs_t.unsqueeze(0))\n",
    "        \n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def get_phi(self, x):\n",
    "        return self.layer3(self.layer2(self.layer1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "reward_buffer = deque([0.0], maxlen=100)\n",
    "\n",
    "all_ranks = deque([0])\n",
    "last_100_ranks = deque([0], maxlen=100)\n",
    "\n",
    "episode_reward = 0.0\n",
    "\n",
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "obs = env.reset()\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    new_obs, reward, done, info = env.step(action)\n",
    "    transition = (obs, action, reward, done, new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = default_loss_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step: 1000\n",
      "Average Reward: 24.097560975609756\n",
      "Average Rank: 18.41\n",
      "\n",
      "Step: 2000\n",
      "Average Reward: 21.021052631578947\n",
      "Average Rank: 20.23\n",
      "\n",
      "Step: 3000\n",
      "Average Reward: 18.2\n",
      "Average Rank: 20.78\n",
      "\n",
      "Step: 4000\n",
      "Average Reward: 19.4\n",
      "Average Rank: 20.61\n",
      "\n",
      "Step: 5000\n",
      "Average Reward: 24.07\n",
      "Average Rank: 20.74\n",
      "\n",
      "Step: 6000\n",
      "Average Reward: 30.84\n",
      "Average Rank: 20.74\n",
      "\n",
      "Step: 7000\n",
      "Average Reward: 36.25\n",
      "Average Rank: 20.66\n",
      "\n",
      "Step: 8000\n",
      "Average Reward: 44.0\n",
      "Average Rank: 21.0\n",
      "\n",
      "Step: 9000\n",
      "Average Reward: 51.93\n",
      "Average Rank: 20.99\n",
      "\n",
      "Step: 10000\n",
      "Average Reward: 59.05\n",
      "Average Rank: 21.0\n",
      "\n",
      "Step: 11000\n",
      "Average Reward: 67.55\n",
      "Average Rank: 21.02\n",
      "\n",
      "Step: 12000\n",
      "Average Reward: 75.28\n",
      "Average Rank: 21.15\n",
      "\n",
      "Step: 13000\n",
      "Average Reward: 82.34\n",
      "Average Rank: 21.73\n",
      "\n",
      "Step: 14000\n",
      "Average Reward: 90.33\n",
      "Average Rank: 21.57\n",
      "\n",
      "Step: 15000\n",
      "Average Reward: 97.54\n",
      "Average Rank: 21.84\n",
      "\n",
      "Step: 16000\n",
      "Average Reward: 105.08\n",
      "Average Rank: 21.71\n",
      "\n",
      "Step: 17000\n",
      "Average Reward: 112.89\n",
      "Average Rank: 21.84\n",
      "\n",
      "Step: 18000\n",
      "Average Reward: 120.4\n",
      "Average Rank: 21.86\n",
      "\n",
      "Step: 19000\n",
      "Average Reward: 128.72\n",
      "Average Rank: 21.99\n",
      "\n",
      "Step: 20000\n",
      "Average Reward: 136.18\n",
      "Average Rank: 21.96\n",
      "\n",
      "Step: 21000\n",
      "Average Reward: 143.03\n",
      "Average Rank: 21.97\n",
      "\n",
      "Step: 22000\n",
      "Average Reward: 149.27\n",
      "Average Rank: 22.0\n",
      "\n",
      "Step: 23000\n",
      "Average Reward: 153.72\n",
      "Average Rank: 22.91\n",
      "\n",
      "Step: 24000\n",
      "Average Reward: 159.63\n",
      "Average Rank: 22.92\n",
      "\n",
      "Step: 25000\n",
      "Average Reward: 164.88\n",
      "Average Rank: 22.91\n",
      "\n",
      "Step: 26000\n",
      "Average Reward: 170.92\n",
      "Average Rank: 22.94\n",
      "\n",
      "Step: 27000\n",
      "Average Reward: 174.41\n",
      "Average Rank: 22.93\n",
      "\n",
      "Step: 28000\n",
      "Average Reward: 179.94\n",
      "Average Rank: 22.93\n",
      "\n",
      "Step: 29000\n",
      "Average Reward: 182.61\n",
      "Average Rank: 22.97\n",
      "\n",
      "Step: 30000\n",
      "Average Reward: 185.27\n",
      "Average Rank: 22.89\n",
      "\n",
      "Step: 31000\n",
      "Average Reward: 186.44\n",
      "Average Rank: 22.97\n",
      "\n",
      "Step: 32000\n",
      "Average Reward: 188.15\n",
      "Average Rank: 22.94\n",
      "\n",
      "Step: 33000\n",
      "Average Reward: 191.86\n",
      "Average Rank: 22.95\n",
      "\n",
      "Step: 34000\n",
      "Average Reward: 193.91\n",
      "Average Rank: 22.93\n",
      "\n",
      "Step: 35000\n",
      "Average Reward: 195.2\n",
      "Average Rank: 22.94\n",
      "\n",
      "Step: 36000\n",
      "Average Reward: 197.1\n",
      "Average Rank: 22.97\n",
      "\n",
      "Step: 37000\n",
      "Average Reward: 197.12\n",
      "Average Rank: 22.94\n",
      "\n",
      "Step: 38000\n",
      "Average Reward: 197.12\n",
      "Average Rank: 22.96\n",
      "\n",
      "Step: 39000\n",
      "Average Reward: 197.12\n",
      "Average Rank: 22.95\n",
      "\n",
      "Step: 40000\n",
      "Average Reward: 197.37\n",
      "Average Rank: 22.9\n",
      "\n",
      "Step: 41000\n",
      "Average Reward: 197.63\n",
      "Average Rank: 22.98\n",
      "\n",
      "Step: 42000\n",
      "Average Reward: 198.43\n",
      "Average Rank: 22.92\n",
      "\n",
      "Step: 43000\n",
      "Average Reward: 199.29\n",
      "Average Rank: 22.9\n",
      "\n",
      "Step: 44000\n",
      "Average Reward: 199.29\n",
      "Average Rank: 22.95\n",
      "\n",
      "Step: 45000\n",
      "Average Reward: 199.35\n",
      "Average Rank: 22.97\n",
      "\n",
      "Step: 46000\n",
      "Average Reward: 199.36\n",
      "Average Rank: 22.94\n",
      "\n",
      "Step: 47000\n",
      "Average Reward: 199.79\n",
      "Average Rank: 22.89\n",
      "\n",
      "Step: 48000\n",
      "Average Reward: 199.79\n",
      "Average Rank: 22.91\n",
      "\n",
      "Step: 49000\n",
      "Average Reward: 199.5\n",
      "Average Rank: 22.98\n",
      "\n",
      "Step: 50000\n",
      "Average Reward: 199.3\n",
      "Average Rank: 22.97\n",
      "\n",
      "Step: 51000\n",
      "Average Reward: 199.04\n",
      "Average Rank: 22.91\n",
      "\n",
      "Step: 52000\n",
      "Average Reward: 199.04\n",
      "Average Rank: 22.96\n",
      "\n",
      "Step: 53000\n",
      "Average Reward: 198.9\n",
      "Average Rank: 22.92\n",
      "\n",
      "Step: 54000\n",
      "Average Reward: 198.89\n",
      "Average Rank: 22.91\n",
      "\n",
      "Step: 55000\n",
      "Average Reward: 198.35\n",
      "Average Rank: 22.82\n",
      "\n",
      "Step: 56000\n",
      "Average Reward: 197.84\n",
      "Average Rank: 22.93\n",
      "\n",
      "Step: 57000\n",
      "Average Reward: 197.16\n",
      "Average Rank: 22.82\n",
      "\n",
      "Step: 58000\n",
      "Average Reward: 197.16\n",
      "Average Rank: 22.87\n",
      "\n",
      "Step: 59000\n",
      "Average Reward: 195.96\n",
      "Average Rank: 22.82\n",
      "\n",
      "Step: 60000\n",
      "Average Reward: 195.83\n",
      "Average Rank: 22.87\n",
      "\n",
      "Step: 61000\n",
      "Average Reward: 195.27\n",
      "Average Rank: 22.77\n",
      "\n",
      "Step: 62000\n",
      "Average Reward: 195.19\n",
      "Average Rank: 22.84\n",
      "\n",
      "Step: 63000\n",
      "Average Reward: 195.19\n",
      "Average Rank: 22.7\n",
      "\n",
      "Step: 64000\n",
      "Average Reward: 194.32\n",
      "Average Rank: 22.69\n",
      "\n",
      "Step: 65000\n",
      "Average Reward: 194.32\n",
      "Average Rank: 22.6\n",
      "\n",
      "Step: 66000\n",
      "Average Reward: 193.1\n",
      "Average Rank: 22.56\n",
      "\n",
      "Step: 67000\n",
      "Average Reward: 193.1\n",
      "Average Rank: 22.56\n",
      "\n",
      "Step: 68000\n",
      "Average Reward: 193.11\n",
      "Average Rank: 22.37\n",
      "\n",
      "Step: 69000\n",
      "Average Reward: 191.92\n",
      "Average Rank: 22.44\n",
      "\n",
      "Step: 70000\n",
      "Average Reward: 192.25\n",
      "Average Rank: 22.31\n",
      "\n",
      "Step: 71000\n",
      "Average Reward: 191.3\n",
      "Average Rank: 22.27\n",
      "\n",
      "Step: 72000\n",
      "Average Reward: 191.3\n",
      "Average Rank: 22.3\n",
      "\n",
      "Step: 73000\n",
      "Average Reward: 190.93\n",
      "Average Rank: 22.15\n",
      "\n",
      "Step: 74000\n",
      "Average Reward: 191.32\n",
      "Average Rank: 22.18\n",
      "\n",
      "Step: 75000\n",
      "Average Reward: 191.17\n",
      "Average Rank: 22.16\n",
      "\n",
      "Step: 76000\n",
      "Average Reward: 191.58\n",
      "Average Rank: 22.14\n",
      "\n",
      "Step: 77000\n",
      "Average Reward: 190.17\n",
      "Average Rank: 22.08\n",
      "\n",
      "Step: 78000\n",
      "Average Reward: 191.37\n",
      "Average Rank: 22.07\n",
      "\n",
      "Step: 79000\n",
      "Average Reward: 190.35\n",
      "Average Rank: 22.01\n",
      "\n",
      "Step: 80000\n",
      "Average Reward: 190.36\n",
      "Average Rank: 22.1\n",
      "\n",
      "Step: 81000\n",
      "Average Reward: 190.36\n",
      "Average Rank: 22.01\n",
      "\n",
      "Step: 82000\n",
      "Average Reward: 190.44\n",
      "Average Rank: 22.05\n",
      "\n",
      "Step: 83000\n",
      "Average Reward: 190.89\n",
      "Average Rank: 21.97\n",
      "\n",
      "Step: 84000\n",
      "Average Reward: 191.27\n",
      "Average Rank: 21.98\n",
      "\n",
      "Step: 85000\n",
      "Average Reward: 191.86\n",
      "Average Rank: 22.01\n",
      "\n",
      "Step: 86000\n",
      "Average Reward: 191.23\n",
      "Average Rank: 22.01\n",
      "\n",
      "Step: 87000\n",
      "Average Reward: 190.84\n",
      "Average Rank: 22.01\n",
      "\n",
      "Step: 88000\n",
      "Average Reward: 191.61\n",
      "Average Rank: 21.97\n",
      "\n",
      "Step: 89000\n",
      "Average Reward: 191.31\n",
      "Average Rank: 21.97\n",
      "\n",
      "Step: 90000\n",
      "Average Reward: 191.48\n",
      "Average Rank: 21.99\n",
      "\n",
      "Step: 91000\n",
      "Average Reward: 191.0\n",
      "Average Rank: 22.0\n",
      "\n",
      "Step: 92000\n",
      "Average Reward: 190.81\n",
      "Average Rank: 21.97\n",
      "\n",
      "Step: 93000\n",
      "Average Reward: 190.39\n",
      "Average Rank: 21.95\n",
      "\n",
      "Step: 94000\n",
      "Average Reward: 189.98\n",
      "Average Rank: 21.97\n",
      "\n",
      "Step: 95000\n",
      "Average Reward: 190.07\n",
      "Average Rank: 21.95\n",
      "\n",
      "Step: 96000\n",
      "Average Reward: 191.27\n",
      "Average Rank: 21.96\n",
      "\n",
      "Step: 97000\n",
      "Average Reward: 191.31\n",
      "Average Rank: 21.95\n",
      "\n",
      "Step: 98000\n",
      "Average Reward: 191.1\n",
      "Average Rank: 21.92\n",
      "\n",
      "Step: 99000\n",
      "Average Reward: 191.65\n",
      "Average Rank: 21.94\n",
      "\n",
      "Step: 100000\n",
      "Average Reward: 191.53\n",
      "Average Rank: 21.96\n",
      "\n",
      "Step: 101000\n",
      "Average Reward: 191.53\n",
      "Average Rank: 21.87\n",
      "\n",
      "Step: 102000\n",
      "Average Reward: 191.12\n",
      "Average Rank: 21.94\n",
      "\n",
      "Step: 103000\n",
      "Average Reward: 190.25\n",
      "Average Rank: 21.86\n",
      "\n",
      "Step: 104000\n",
      "Average Reward: 189.56\n",
      "Average Rank: 21.8\n",
      "\n",
      "Step: 105000\n",
      "Average Reward: 188.11\n",
      "Average Rank: 21.86\n",
      "\n",
      "Step: 106000\n",
      "Average Reward: 185.82\n",
      "Average Rank: 21.79\n",
      "\n",
      "Step: 107000\n",
      "Average Reward: 182.23\n",
      "Average Rank: 21.93\n",
      "\n",
      "Step: 108000\n",
      "Average Reward: 181.37\n",
      "Average Rank: 21.84\n",
      "\n",
      "Step: 109000\n",
      "Average Reward: 180.6\n",
      "Average Rank: 21.89\n",
      "\n",
      "Step: 110000\n",
      "Average Reward: 180.6\n",
      "Average Rank: 21.81\n",
      "\n",
      "Step: 111000\n",
      "Average Reward: 180.09\n",
      "Average Rank: 21.83\n",
      "\n",
      "Step: 112000\n",
      "Average Reward: 179.85\n",
      "Average Rank: 21.88\n",
      "\n",
      "Step: 113000\n",
      "Average Reward: 179.76\n",
      "Average Rank: 21.84\n",
      "\n",
      "Step: 114000\n",
      "Average Reward: 179.86\n",
      "Average Rank: 21.71\n",
      "\n",
      "Step: 115000\n",
      "Average Reward: 178.1\n",
      "Average Rank: 21.94\n",
      "\n",
      "Step: 116000\n",
      "Average Reward: 176.3\n",
      "Average Rank: 21.88\n",
      "\n",
      "Step: 117000\n",
      "Average Reward: 167.75\n",
      "Average Rank: 21.92\n",
      "\n",
      "Step: 118000\n",
      "Average Reward: 166.51\n",
      "Average Rank: 21.92\n",
      "\n",
      "Step: 119000\n",
      "Average Reward: 161.61\n",
      "Average Rank: 21.98\n",
      "\n",
      "Step: 120000\n",
      "Average Reward: 159.41\n",
      "Average Rank: 21.98\n",
      "\n",
      "Step: 121000\n",
      "Average Reward: 155.98\n",
      "Average Rank: 22.08\n",
      "\n",
      "Step: 122000\n",
      "Average Reward: 158.7\n",
      "Average Rank: 22.06\n",
      "\n",
      "Step: 123000\n",
      "Average Reward: 160.04\n",
      "Average Rank: 22.0\n",
      "\n",
      "Step: 124000\n",
      "Average Reward: 159.82\n",
      "Average Rank: 22.05\n",
      "\n",
      "Step: 125000\n",
      "Average Reward: 159.57\n",
      "Average Rank: 22.09\n",
      "\n",
      "Step: 126000\n",
      "Average Reward: 159.49\n",
      "Average Rank: 22.11\n",
      "\n",
      "Step: 127000\n",
      "Average Reward: 159.78\n",
      "Average Rank: 22.2\n",
      "\n",
      "Step: 128000\n",
      "Average Reward: 160.24\n",
      "Average Rank: 22.15\n",
      "\n",
      "Step: 129000\n",
      "Average Reward: 158.55\n",
      "Average Rank: 22.09\n",
      "\n",
      "Step: 130000\n",
      "Average Reward: 157.17\n",
      "Average Rank: 22.22\n",
      "\n",
      "Step: 131000\n",
      "Average Reward: 158.2\n",
      "Average Rank: 22.23\n",
      "\n",
      "Step: 132000\n",
      "Average Reward: 158.39\n",
      "Average Rank: 22.25\n",
      "\n",
      "Step: 133000\n",
      "Average Reward: 163.07\n",
      "Average Rank: 22.22\n",
      "\n",
      "Step: 134000\n",
      "Average Reward: 166.8\n",
      "Average Rank: 22.24\n",
      "\n",
      "Step: 135000\n",
      "Average Reward: 166.71\n",
      "Average Rank: 22.21\n",
      "\n",
      "Step: 136000\n",
      "Average Reward: 170.89\n",
      "Average Rank: 22.2\n",
      "\n",
      "Step: 137000\n",
      "Average Reward: 171.24\n",
      "Average Rank: 22.0\n",
      "\n",
      "Step: 138000\n",
      "Average Reward: 168.47\n",
      "Average Rank: 22.07\n",
      "\n",
      "Step: 139000\n",
      "Average Reward: 157.64\n",
      "Average Rank: 22.25\n",
      "\n",
      "Step: 140000\n",
      "Average Reward: 154.9\n",
      "Average Rank: 22.27\n",
      "\n",
      "Step: 141000\n",
      "Average Reward: 154.71\n",
      "Average Rank: 22.25\n",
      "\n",
      "Step: 142000\n",
      "Average Reward: 154.17\n",
      "Average Rank: 22.28\n",
      "\n",
      "Step: 143000\n",
      "Average Reward: 153.37\n",
      "Average Rank: 22.09\n",
      "\n",
      "Step: 144000\n",
      "Average Reward: 154.33\n",
      "Average Rank: 21.92\n",
      "\n",
      "Step: 145000\n",
      "Average Reward: 154.38\n",
      "Average Rank: 22.12\n",
      "\n",
      "Step: 146000\n",
      "Average Reward: 154.91\n",
      "Average Rank: 22.19\n",
      "\n",
      "Step: 147000\n",
      "Average Reward: 155.95\n",
      "Average Rank: 21.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step: 148000\n",
      "Average Reward: 157.92\n",
      "Average Rank: 21.69\n",
      "\n",
      "Step: 149000\n",
      "Average Reward: 158.54\n",
      "Average Rank: 21.91\n",
      "\n",
      "Step: 150000\n",
      "Average Reward: 158.84\n",
      "Average Rank: 21.82\n",
      "\n",
      "Step: 151000\n",
      "Average Reward: 159.19\n",
      "Average Rank: 21.88\n",
      "\n",
      "Step: 152000\n",
      "Average Reward: 160.73\n",
      "Average Rank: 21.84\n",
      "\n",
      "Step: 153000\n",
      "Average Reward: 160.08\n",
      "Average Rank: 21.89\n",
      "\n",
      "Step: 154000\n",
      "Average Reward: 164.46\n",
      "Average Rank: 21.89\n",
      "\n",
      "Step: 155000\n",
      "Average Reward: 169.99\n",
      "Average Rank: 21.83\n",
      "\n",
      "Step: 156000\n",
      "Average Reward: 175.34\n",
      "Average Rank: 21.87\n",
      "\n",
      "Step: 157000\n",
      "Average Reward: 181.41\n",
      "Average Rank: 21.85\n",
      "\n",
      "Step: 158000\n",
      "Average Reward: 186.82\n",
      "Average Rank: 21.86\n",
      "\n",
      "Step: 159000\n",
      "Average Reward: 189.92\n",
      "Average Rank: 21.93\n",
      "\n",
      "Step: 160000\n",
      "Average Reward: 191.13\n",
      "Average Rank: 21.67\n",
      "\n",
      "Step: 161000\n",
      "Average Reward: 192.05\n",
      "Average Rank: 21.73\n",
      "\n",
      "Step: 162000\n",
      "Average Reward: 193.55\n",
      "Average Rank: 21.77\n",
      "\n",
      "Step: 163000\n",
      "Average Reward: 194.32\n",
      "Average Rank: 21.67\n",
      "\n",
      "Step: 164000\n",
      "Average Reward: 195.17\n",
      "Average Rank: 21.68\n",
      "\n",
      "Step: 165000\n",
      "Average Reward: 196.92\n",
      "Average Rank: 21.74\n",
      "\n",
      "Step: 166000\n",
      "Average Reward: 197.04\n",
      "Average Rank: 21.69\n",
      "\n",
      "Step: 167000\n",
      "Average Reward: 196.86\n",
      "Average Rank: 21.62\n",
      "\n",
      "Step: 168000\n",
      "Average Reward: 197.1\n",
      "Average Rank: 21.57\n",
      "\n",
      "Step: 169000\n",
      "Average Reward: 197.1\n",
      "Average Rank: 21.5\n",
      "\n",
      "Step: 170000\n",
      "Average Reward: 197.1\n",
      "Average Rank: 21.55\n",
      "\n",
      "Step: 171000\n",
      "Average Reward: 197.23\n",
      "Average Rank: 21.46\n",
      "\n",
      "Step: 172000\n",
      "Average Reward: 197.23\n",
      "Average Rank: 21.44\n",
      "\n",
      "Step: 173000\n",
      "Average Reward: 198.02\n",
      "Average Rank: 21.37\n",
      "\n",
      "Step: 174000\n",
      "Average Reward: 198.02\n",
      "Average Rank: 21.4\n",
      "\n",
      "Step: 175000\n",
      "Average Reward: 198.71\n",
      "Average Rank: 21.34\n",
      "\n",
      "Step: 176000\n",
      "Average Reward: 199.57\n",
      "Average Rank: 21.39\n",
      "\n",
      "Step: 177000\n",
      "Average Reward: 199.57\n",
      "Average Rank: 21.37\n",
      "\n",
      "Step: 178000\n",
      "Average Reward: 199.57\n",
      "Average Rank: 21.43\n",
      "\n",
      "Step: 179000\n",
      "Average Reward: 199.57\n",
      "Average Rank: 21.36\n",
      "\n",
      "Step: 180000\n",
      "Average Reward: 199.57\n",
      "Average Rank: 21.41\n",
      "\n",
      "Step: 181000\n",
      "Average Reward: 199.57\n",
      "Average Rank: 21.37\n",
      "\n",
      "Step: 182000\n",
      "Average Reward: 199.57\n",
      "Average Rank: 21.39\n",
      "\n",
      "Step: 183000\n",
      "Average Reward: 199.57\n",
      "Average Rank: 21.43\n",
      "\n",
      "Step: 184000\n",
      "Average Reward: 199.57\n",
      "Average Rank: 21.38\n",
      "\n",
      "Step: 185000\n",
      "Average Reward: 199.57\n",
      "Average Rank: 21.54\n",
      "\n",
      "Step: 186000\n",
      "Average Reward: 199.63\n",
      "Average Rank: 21.42\n",
      "\n",
      "Step: 187000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.45\n",
      "\n",
      "Step: 188000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.43\n",
      "\n",
      "Step: 189000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.14\n",
      "\n",
      "Step: 190000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.05\n",
      "\n",
      "Step: 191000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.01\n",
      "\n",
      "Step: 192000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.03\n",
      "\n",
      "Step: 193000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.01\n",
      "\n",
      "Step: 194000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.0\n",
      "\n",
      "Step: 195000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.01\n",
      "\n",
      "Step: 196000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.01\n",
      "\n",
      "Step: 197000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.02\n",
      "\n",
      "Step: 198000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.01\n",
      "\n",
      "Step: 199000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.0\n",
      "\n",
      "Step: 200000\n",
      "Average Reward: 200.0\n",
      "Average Rank: 21.0\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in range(1, MAX_STEPS+1):\n",
    "    \n",
    "    epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END])\n",
    "    \n",
    "    rng = random.random()\n",
    "    if rng <= epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = online_net.act(obs)\n",
    "    \n",
    "    new_obs, reward, done, _ = env.step(action)\n",
    "    transition = (obs, action, reward, done, new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "    \n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        reward_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "    \n",
    "    # watch play\n",
    "    if len(reward_buffer) >= 200:\n",
    "        if np.mean(reward_buffer) >= 195: # once model averages score of 195 (max 200)\n",
    "            while True:\n",
    "                action = online_net.act(obs)\n",
    "                \n",
    "                obs, _, done, _ = env.step(action)\n",
    "                env.render()\n",
    "                if done:\n",
    "                    env.reset()\n",
    "        \n",
    "    # Start Gradient Step   \n",
    "    transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "    \n",
    "    observations = np.asarray([t[0] for t in transitions])\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    rewards = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_observations = np.asarray([t[4] for t in transitions])\n",
    "    \n",
    "    observations_t = torch.as_tensor(observations, dtype=torch.float32)\n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_t = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_observations_t = torch.as_tensor(new_observations, dtype=torch.float32)\n",
    "    \n",
    "    # Compute Targets\n",
    "    target_q_values = target_net(new_observations_t)\n",
    "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "    \n",
    "    targets = rewards_t + GAMMA * (1 - dones_t) * max_target_q_values\n",
    "    \n",
    "    # Compute Loss\n",
    "    q_values = online_net(observations_t)\n",
    "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
    "    loss = loss_function(action_q_values, targets)\n",
    "    \n",
    "    # Gradient Descent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Compute Phi Matrix\n",
    "    phi_matrix = []   \n",
    "    for i in range(len(observations_t)):\n",
    "        row = online_net.get_phi(observations_t[i])\n",
    "        row = torch.Tensor.tolist(row)\n",
    "        phi_matrix.append(row)\n",
    "    phi_matrix = np.array(phi_matrix)\n",
    "    rank = np.linalg.matrix_rank(phi_matrix)\n",
    "    all_ranks.append(rank)\n",
    "    last_100_ranks.append(rank)\n",
    "    \n",
    "    # Update Target Network\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "        \n",
    "    # Logging\n",
    "    if step % 1000 == 0:\n",
    "        print()\n",
    "        print(\"Step:\", step)\n",
    "        print(\"Average Reward:\", np.mean(reward_buffer))\n",
    "        print(\"Average Rank:\", np.mean(last_100_ranks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
