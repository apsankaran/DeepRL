{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Implementation - PyTorch V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 50000\n",
    "MIN_REPLAY_SIZE = 1000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY = 10000\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "LEARNING_RATE = 5e-4\n",
    "MAX_STEPS = 200000\n",
    "\n",
    "# Regularization Coefficient\n",
    "REGULARIZATION_COEFFICIENT = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_matrix = []\n",
    "observations_t = None\n",
    "new_observations_t = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default loss function - mean squared error\n",
    "\n",
    "def default_loss_mse(y_true, y_pred):\n",
    "    \n",
    "    loss = torch.mean(torch.square(y_true-y_pred))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function - implements explicit DR3 regularizer\n",
    "\n",
    "# add dot product between each state action and subsequent oneâ€™s feature vector to loss\n",
    "def dr3(y_true, y_pred):\n",
    "    \n",
    "    global observations_t, new_observations_t\n",
    "    \n",
    "    loss = torch.mean(torch.square(y_true-y_pred))\n",
    "    \n",
    "    if observations_t != None and new_observations_t != None:\n",
    "        \n",
    "        for i in range(len(observations_t)):          \n",
    "            curr_state_feature_vector = online_net.get_phi(observations_t[i]).cpu().detach().numpy()\n",
    "            next_state_feature_vector = online_net.get_phi(new_observations_t[i]).cpu().detach().numpy()\n",
    "            loss += REGULARIZATION_COEFFICIENT * np.dot(curr_state_feature_vector, next_state_feature_vector)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function - random dot product from phi matrix\n",
    "\n",
    "# randomly sample two vectors from the phi matrix and add dot product of those vectors to loss\n",
    "def random_dot(y_true, y_pred):\n",
    "\n",
    "    global phi_matrix\n",
    "    \n",
    "    loss = torch.mean(torch.square(y_true-y_pred))\n",
    "    \n",
    "    # Explicit Regularization\n",
    "    if ((phi_matrix is not None) and (len(phi_matrix) > 1)):\n",
    "        \n",
    "        v1 = phi_matrix[random.randrange(len(phi_matrix))]\n",
    "        v2 = phi_matrix[random.randrange(len(phi_matrix))]\n",
    "        \n",
    "        loss += REGULARIZATION_COEFFICIENT * np.dot(np.array(v1), np.array(v2))\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function - implements regulizer based on min/max singular values in phi matrix\n",
    "\n",
    "# add difference between max entry in phi matrix ** 2 and min entry in phi matrix ** 2 to loss\n",
    "def phi_penalty(y_true, y_pred):\n",
    "    \n",
    "    global phi_matrix\n",
    "    \n",
    "    loss = torch.mean(torch.square(y_true-y_pred))\n",
    "    \n",
    "    # Explicit Regularization\n",
    "    if ((phi_matrix is not None) and (len(phi_matrix) > 0)):\n",
    "        minimum = min([min(value) for value in phi_matrix])\n",
    "        maximum = max([max(value) for value in phi_matrix])\n",
    "        loss += REGULARIZATION_COEFFICIENT * (maximum**2 - minimum**2)\n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network class\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super().__init__()        \n",
    "        in_features = int(np.prod(env.observation_space.shape))     \n",
    "        # Neural Network\n",
    "        self.layer1 = nn.Linear(in_features, 24)\n",
    "        self.layer2 = nn.ReLU()\n",
    "        self.layer3 = nn.ReLU()\n",
    "        self.layer4 = nn.Linear(24, env.action_space.n)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer4(self.layer3(self.layer2(self.layer1(x))))\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self(obs_t.unsqueeze(0))\n",
    "        \n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def get_phi(self, x):\n",
    "        return self.layer3(self.layer2(self.layer1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "reward_buffer = deque([0.0], maxlen=100)\n",
    "\n",
    "all_ranks = deque([0])\n",
    "last_100_ranks = deque([0], maxlen=100)\n",
    "\n",
    "episode_reward = 0.0\n",
    "\n",
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "obs = env.reset()\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    new_obs, reward, done, info = env.step(action)\n",
    "    transition = (obs, action, reward, done, new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = default_loss_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step: 1000\n",
      "Average Reward: 24.097560975609756\n",
      "Average Rank: 18.41\n",
      "\n",
      "Step: 2000\n",
      "Average Reward: 21.021052631578947\n",
      "Average Rank: 20.23\n",
      "\n",
      "Step: 3000\n",
      "Average Reward: 18.2\n",
      "Average Rank: 20.78\n",
      "\n",
      "Step: 4000\n",
      "Average Reward: 19.4\n",
      "Average Rank: 20.61\n",
      "\n",
      "Step: 5000\n",
      "Average Reward: 24.07\n",
      "Average Rank: 20.74\n",
      "\n",
      "Step: 6000\n",
      "Average Reward: 30.84\n",
      "Average Rank: 20.74\n",
      "\n",
      "Step: 7000\n",
      "Average Reward: 36.25\n",
      "Average Rank: 20.66\n",
      "\n",
      "Step: 8000\n",
      "Average Reward: 44.0\n",
      "Average Rank: 21.0\n",
      "\n",
      "Step: 9000\n",
      "Average Reward: 51.93\n",
      "Average Rank: 20.99\n",
      "\n",
      "Step: 10000\n",
      "Average Reward: 59.05\n",
      "Average Rank: 21.0\n",
      "\n",
      "Step: 11000\n",
      "Average Reward: 67.55\n",
      "Average Rank: 21.02\n",
      "\n",
      "Step: 12000\n",
      "Average Reward: 75.28\n",
      "Average Rank: 21.15\n",
      "\n",
      "Step: 13000\n",
      "Average Reward: 82.34\n",
      "Average Rank: 21.73\n",
      "\n",
      "Step: 14000\n",
      "Average Reward: 90.33\n",
      "Average Rank: 21.57\n",
      "\n",
      "Step: 15000\n",
      "Average Reward: 97.54\n",
      "Average Rank: 21.84\n",
      "\n",
      "Step: 16000\n",
      "Average Reward: 105.08\n",
      "Average Rank: 21.71\n",
      "\n",
      "Step: 17000\n",
      "Average Reward: 112.89\n",
      "Average Rank: 21.84\n",
      "\n",
      "Step: 18000\n",
      "Average Reward: 120.4\n",
      "Average Rank: 21.86\n",
      "\n",
      "Step: 19000\n",
      "Average Reward: 128.72\n",
      "Average Rank: 21.99\n",
      "\n",
      "Step: 20000\n",
      "Average Reward: 136.18\n",
      "Average Rank: 21.96\n",
      "\n",
      "Step: 21000\n",
      "Average Reward: 143.03\n",
      "Average Rank: 21.97\n",
      "\n",
      "Step: 22000\n",
      "Average Reward: 149.27\n",
      "Average Rank: 22.0\n",
      "\n",
      "Step: 23000\n",
      "Average Reward: 153.72\n",
      "Average Rank: 22.91\n",
      "\n",
      "Step: 24000\n",
      "Average Reward: 159.63\n",
      "Average Rank: 22.92\n",
      "\n",
      "Step: 25000\n",
      "Average Reward: 164.88\n",
      "Average Rank: 22.91\n",
      "\n",
      "Step: 26000\n",
      "Average Reward: 170.92\n",
      "Average Rank: 22.94\n",
      "\n",
      "Step: 27000\n",
      "Average Reward: 174.41\n",
      "Average Rank: 22.93\n",
      "\n",
      "Step: 28000\n",
      "Average Reward: 179.94\n",
      "Average Rank: 22.93\n",
      "\n",
      "Step: 29000\n",
      "Average Reward: 182.61\n",
      "Average Rank: 22.97\n",
      "\n",
      "Step: 30000\n",
      "Average Reward: 185.27\n",
      "Average Rank: 22.89\n",
      "\n",
      "Step: 31000\n",
      "Average Reward: 186.44\n",
      "Average Rank: 22.97\n",
      "\n",
      "Step: 32000\n",
      "Average Reward: 188.15\n",
      "Average Rank: 22.94\n",
      "\n",
      "Step: 33000\n",
      "Average Reward: 191.86\n",
      "Average Rank: 22.95\n",
      "\n",
      "Step: 34000\n",
      "Average Reward: 193.91\n",
      "Average Rank: 22.93\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in range(1, MAX_STEPS+1):\n",
    "    \n",
    "    epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END])\n",
    "    \n",
    "    rng = random.random()\n",
    "    if rng <= epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = online_net.act(obs)\n",
    "    \n",
    "    new_obs, reward, done, _ = env.step(action)\n",
    "    transition = (obs, action, reward, done, new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "    \n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        reward_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "    \n",
    "    # watch play\n",
    "    if len(reward_buffer) >= 200:\n",
    "        if np.mean(reward_buffer) >= 195: # once model averages score of 195 (max 200)\n",
    "            while True:\n",
    "                action = online_net.act(obs)\n",
    "                \n",
    "                obs, _, done, _ = env.step(action)\n",
    "                env.render()\n",
    "                if done:\n",
    "                    env.reset()\n",
    "        \n",
    "    # Start Gradient Step   \n",
    "    transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "    \n",
    "    observations = np.asarray([t[0] for t in transitions])\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    rewards = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_observations = np.asarray([t[4] for t in transitions])\n",
    "    \n",
    "    observations_t = torch.as_tensor(observations, dtype=torch.float32)\n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_t = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_observations_t = torch.as_tensor(new_observations, dtype=torch.float32)\n",
    "    \n",
    "    # Compute Targets\n",
    "    target_q_values = target_net(new_observations_t)\n",
    "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "    \n",
    "    targets = rewards_t + GAMMA * (1 - dones_t) * max_target_q_values\n",
    "    \n",
    "    # Compute Loss\n",
    "    q_values = online_net(observations_t)\n",
    "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
    "    loss = loss_function(action_q_values, targets)\n",
    "    \n",
    "    # Gradient Descent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Compute Phi Matrix\n",
    "    phi_matrix = []   \n",
    "    for i in range(len(observations_t)):\n",
    "        row = online_net.get_phi(observations_t[i])\n",
    "        row = torch.Tensor.tolist(row)\n",
    "        phi_matrix.append(row)\n",
    "    phi_matrix = np.array(phi_matrix)\n",
    "    rank = np.linalg.matrix_rank(phi_matrix)\n",
    "    all_ranks.append(rank)\n",
    "    last_100_ranks.append(rank)\n",
    "    \n",
    "    # Update Target Network\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "        \n",
    "    # Logging\n",
    "    if step % 1000 == 0:\n",
    "        print()\n",
    "        print(\"Step:\", step)\n",
    "        print(\"Average Reward:\", np.mean(reward_buffer))\n",
    "        print(\"Average Rank:\", np.mean(last_100_ranks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
