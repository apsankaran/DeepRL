{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 50000\n",
    "MIN_REPLAY_SIZE = 1000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY = 10000\n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "LEARNING_RATE = 5e-4\n",
    "MAX_STEPS = 200000\n",
    "\n",
    "# Regularization Coefficient\n",
    "REGULARIZATION_COEFFICIENT = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_matrix = []\n",
    "observations_t = None\n",
    "new_observations_t = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default loss function - mean squared error\n",
    "\n",
    "def default_loss_mse(y_true, y_pred):\n",
    "    \n",
    "    loss = torch.mean(torch.square(y_true-y_pred))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function - implements explicit DR3 regularizer\n",
    "\n",
    "# add dot product between each state action and subsequent oneâ€™s feature vector to loss\n",
    "def dr3(y_true, y_pred):\n",
    "    \n",
    "    global observations_t, new_observations_t\n",
    "    \n",
    "    loss = torch.mean(torch.square(y_true-y_pred))\n",
    "    \n",
    "    if observations_t != None and new_observations_t != None:\n",
    "        \n",
    "        for i in range(len(observations_t)):          \n",
    "            curr_state_feature_vector = online_net.get_phi(observations_t[i]).cpu().detach().numpy()\n",
    "            next_state_feature_vector = online_net.get_phi(new_observations_t[i]).cpu().detach().numpy()\n",
    "            loss += REGULARIZATION_COEFFICIENT * np.dot(curr_state_feature_vector, next_state_feature_vector)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function - random dot product from phi matrix\n",
    "\n",
    "# randomly sample two vectors from the phi matrix and add dot product of those vectors to loss\n",
    "def random_dot(y_true, y_pred):\n",
    "\n",
    "    global phi_matrix\n",
    "    \n",
    "    loss = torch.mean(torch.square(y_true-y_pred))\n",
    "    \n",
    "    # Explicit Regularization\n",
    "    if ((phi_matrix is not None) and (len(phi_matrix) > 1)):\n",
    "        \n",
    "        v1 = phi_matrix[random.randrange(len(phi_matrix))]\n",
    "        v2 = phi_matrix[random.randrange(len(phi_matrix))]\n",
    "        \n",
    "        loss += REGULARIZATION_COEFFICIENT * np.dot(np.array(v1), np.array(v2))\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss function - implements regulizer based on min/max singular values in phi matrix\n",
    "\n",
    "# add difference between max entry in phi matrix ** 2 and min entry in phi matrix ** 2 to loss\n",
    "def phi_penalty(y_true, y_pred):\n",
    "    \n",
    "    global phi_matrix\n",
    "    \n",
    "    loss = torch.mean(torch.square(y_true-y_pred))\n",
    "    \n",
    "    # Explicit Regularization\n",
    "    if ((phi_matrix is not None) and (len(phi_matrix) > 0)):\n",
    "        minimum = min([min(value) for value in phi_matrix])\n",
    "        maximum = max([max(value) for value in phi_matrix])\n",
    "        loss += REGULARIZATION_COEFFICIENT * (maximum**2 - minimum**2)\n",
    "            \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network class\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super().__init__()        \n",
    "        in_features = int(np.prod(env.observation_space.shape))     \n",
    "        # Neural Network\n",
    "        self.layer1 = nn.Linear(in_features, 24)\n",
    "        self.layer2 = nn.ReLU()\n",
    "        self.layer3 = nn.ReLU()\n",
    "        self.layer4 = nn.Linear(24, env.action_space.n)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer4(self.layer3(self.layer2(self.layer1(x))))\n",
    "    \n",
    "    def act(self, obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self(obs_t.unsqueeze(0))\n",
    "        \n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def get_phi(self, x):\n",
    "        return self.layer3(self.layer2(self.layer1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OpenAI Gym Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "reward_buffer = deque([0.0], maxlen=100)\n",
    "\n",
    "all_ranks = deque([0])\n",
    "last_100_ranks = deque([0], maxlen=100)\n",
    "\n",
    "episode_reward = 0.0\n",
    "\n",
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "obs = env.reset()\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    \n",
    "    action = env.action_space.sample()\n",
    "    new_obs, reward, done, info = env.step(action)\n",
    "    transition = (obs, action, reward, done, new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_loss_mse\n",
    "# dr3\n",
    "# random_dot\n",
    "# phi_pentalty\n",
    "\n",
    "loss_function = default_loss_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step: 1000\n",
      "Average Reward: 19.235294117647058\n",
      "Average Rank: 16.41\n",
      "\n",
      "Step: 2000\n",
      "Average Reward: 19.83\n",
      "Average Rank: 16.37\n",
      "\n",
      "Step: 3000\n",
      "Average Reward: 20.32\n",
      "Average Rank: 15.32\n",
      "\n",
      "Step: 4000\n",
      "Average Reward: 19.18\n",
      "Average Rank: 14.42\n",
      "\n",
      "Step: 5000\n",
      "Average Reward: 22.73\n",
      "Average Rank: 14.07\n",
      "\n",
      "Step: 6000\n",
      "Average Reward: 29.56\n",
      "Average Rank: 14.0\n",
      "\n",
      "Step: 7000\n",
      "Average Reward: 37.08\n",
      "Average Rank: 13.96\n",
      "\n",
      "Step: 8000\n",
      "Average Reward: 44.62\n",
      "Average Rank: 13.79\n",
      "\n",
      "Step: 9000\n",
      "Average Reward: 53.14\n",
      "Average Rank: 13.55\n",
      "\n",
      "Step: 10000\n",
      "Average Reward: 60.41\n",
      "Average Rank: 13.37\n",
      "\n",
      "Step: 11000\n",
      "Average Reward: 69.58\n",
      "Average Rank: 13.29\n",
      "\n",
      "Step: 12000\n",
      "Average Reward: 77.87\n",
      "Average Rank: 13.22\n",
      "\n",
      "Step: 13000\n",
      "Average Reward: 86.27\n",
      "Average Rank: 13.21\n",
      "\n",
      "Step: 14000\n",
      "Average Reward: 94.62\n",
      "Average Rank: 13.9\n",
      "\n",
      "Step: 15000\n",
      "Average Reward: 102.22\n",
      "Average Rank: 13.85\n",
      "\n",
      "Step: 16000\n",
      "Average Reward: 108.78\n",
      "Average Rank: 13.68\n",
      "\n",
      "Step: 17000\n",
      "Average Reward: 118.25\n",
      "Average Rank: 13.67\n",
      "\n",
      "Step: 18000\n",
      "Average Reward: 123.75\n",
      "Average Rank: 13.5\n",
      "\n",
      "Step: 19000\n",
      "Average Reward: 130.92\n",
      "Average Rank: 13.48\n",
      "\n",
      "Step: 20000\n",
      "Average Reward: 138.16\n",
      "Average Rank: 13.4\n",
      "\n",
      "Step: 21000\n",
      "Average Reward: 144.56\n",
      "Average Rank: 13.37\n",
      "\n",
      "Step: 22000\n",
      "Average Reward: 151.19\n",
      "Average Rank: 13.29\n",
      "\n",
      "Step: 23000\n",
      "Average Reward: 157.61\n",
      "Average Rank: 13.16\n",
      "\n",
      "Step: 24000\n",
      "Average Reward: 162.96\n",
      "Average Rank: 13.06\n",
      "\n",
      "Step: 25000\n",
      "Average Reward: 169.16\n",
      "Average Rank: 13.14\n",
      "\n",
      "Step: 26000\n",
      "Average Reward: 174.25\n",
      "Average Rank: 12.93\n",
      "\n",
      "Step: 27000\n",
      "Average Reward: 179.44\n",
      "Average Rank: 13.08\n",
      "\n",
      "Step: 28000\n",
      "Average Reward: 184.2\n",
      "Average Rank: 12.97\n",
      "\n",
      "Step: 29000\n",
      "Average Reward: 188.9\n",
      "Average Rank: 12.91\n",
      "\n",
      "Step: 30000\n",
      "Average Reward: 191.31\n",
      "Average Rank: 13.0\n",
      "\n",
      "Step: 31000\n",
      "Average Reward: 193.24\n",
      "Average Rank: 12.93\n",
      "\n",
      "Step: 32000\n",
      "Average Reward: 194.1\n",
      "Average Rank: 12.89\n",
      "\n",
      "Step: 33000\n",
      "Average Reward: 194.96\n",
      "Average Rank: 12.84\n",
      "\n",
      "Step: 34000\n",
      "Average Reward: 195.86\n",
      "Average Rank: 12.8\n",
      "\n",
      "Step: 35000\n",
      "Average Reward: 196.34\n",
      "Average Rank: 12.81\n",
      "\n",
      "Step: 36000\n",
      "Average Reward: 196.84\n",
      "Average Rank: 12.86\n",
      "\n",
      "Step: 37000\n",
      "Average Reward: 197.27\n",
      "Average Rank: 12.78\n",
      "\n",
      "Step: 38000\n",
      "Average Reward: 197.93\n",
      "Average Rank: 12.8\n",
      "\n",
      "Step: 39000\n",
      "Average Reward: 199.05\n",
      "Average Rank: 12.76\n",
      "\n",
      "Step: 40000\n",
      "Average Reward: 199.35\n",
      "Average Rank: 12.83\n",
      "\n",
      "Step: 41000\n",
      "Average Reward: 199.26\n",
      "Average Rank: 12.71\n",
      "\n",
      "Step: 42000\n",
      "Average Reward: 199.43\n",
      "Average Rank: 12.77\n",
      "\n",
      "Step: 43000\n",
      "Average Reward: 199.51\n",
      "Average Rank: 12.72\n",
      "\n",
      "Step: 44000\n",
      "Average Reward: 199.51\n",
      "Average Rank: 12.7\n",
      "\n",
      "Step: 45000\n",
      "Average Reward: 199.51\n",
      "Average Rank: 12.8\n",
      "\n",
      "Step: 46000\n",
      "Average Reward: 199.51\n",
      "Average Rank: 12.62\n",
      "\n",
      "Step: 47000\n",
      "Average Reward: 199.51\n",
      "Average Rank: 12.56\n",
      "\n",
      "Step: 48000\n",
      "Average Reward: 199.75\n",
      "Average Rank: 12.59\n",
      "\n",
      "Step: 49000\n",
      "Average Reward: 199.75\n",
      "Average Rank: 12.57\n",
      "\n",
      "Step: 50000\n",
      "Average Reward: 199.75\n",
      "Average Rank: 12.37\n",
      "\n",
      "Step: 51000\n",
      "Average Reward: 199.75\n",
      "Average Rank: 12.24\n",
      "\n",
      "Step: 52000\n",
      "Average Reward: 199.75\n",
      "Average Rank: 12.24\n",
      "\n",
      "Step: 53000\n",
      "Average Reward: 199.75\n",
      "Average Rank: 12.28\n",
      "\n",
      "Step: 54000\n",
      "Average Reward: 199.75\n",
      "Average Rank: 12.07\n",
      "\n",
      "Step: 55000\n",
      "Average Reward: 199.75\n",
      "Average Rank: 11.73\n",
      "\n",
      "Step: 56000\n",
      "Average Reward: 199.53\n",
      "Average Rank: 11.54\n",
      "\n",
      "Step: 57000\n",
      "Average Reward: 199.31\n",
      "Average Rank: 11.54\n",
      "\n",
      "Step: 58000\n",
      "Average Reward: 199.31\n",
      "Average Rank: 11.35\n",
      "\n",
      "Step: 59000\n",
      "Average Reward: 199.31\n",
      "Average Rank: 11.36\n",
      "\n",
      "Step: 60000\n",
      "Average Reward: 199.23\n",
      "Average Rank: 11.26\n",
      "\n",
      "Step: 61000\n",
      "Average Reward: 199.32\n",
      "Average Rank: 11.03\n",
      "\n",
      "Step: 62000\n",
      "Average Reward: 199.48\n",
      "Average Rank: 11.03\n",
      "\n",
      "Step: 63000\n",
      "Average Reward: 199.3\n",
      "Average Rank: 11.11\n",
      "\n",
      "Step: 64000\n",
      "Average Reward: 198.95\n",
      "Average Rank: 11.05\n",
      "\n",
      "Step: 65000\n",
      "Average Reward: 198.95\n",
      "Average Rank: 11.05\n",
      "\n",
      "Step: 66000\n",
      "Average Reward: 198.79\n",
      "Average Rank: 11.02\n",
      "\n",
      "Step: 67000\n",
      "Average Reward: 198.79\n",
      "Average Rank: 11.02\n",
      "\n",
      "Step: 68000\n",
      "Average Reward: 198.79\n",
      "Average Rank: 10.88\n",
      "\n",
      "Step: 69000\n",
      "Average Reward: 198.72\n",
      "Average Rank: 10.9\n",
      "\n",
      "Step: 70000\n",
      "Average Reward: 198.72\n",
      "Average Rank: 10.96\n",
      "\n",
      "Step: 71000\n",
      "Average Reward: 198.72\n",
      "Average Rank: 10.9\n",
      "\n",
      "Step: 72000\n",
      "Average Reward: 198.72\n",
      "Average Rank: 10.9\n",
      "\n",
      "Step: 73000\n",
      "Average Reward: 198.72\n",
      "Average Rank: 10.85\n",
      "\n",
      "Step: 74000\n",
      "Average Reward: 198.5\n",
      "Average Rank: 10.9\n",
      "\n",
      "Step: 75000\n",
      "Average Reward: 198.43\n",
      "Average Rank: 10.83\n",
      "\n",
      "Step: 76000\n",
      "Average Reward: 198.65\n",
      "Average Rank: 10.91\n",
      "\n",
      "Step: 77000\n",
      "Average Reward: 198.76\n",
      "Average Rank: 10.92\n",
      "\n",
      "Step: 78000\n",
      "Average Reward: 198.63\n",
      "Average Rank: 10.89\n",
      "\n",
      "Step: 79000\n",
      "Average Reward: 198.12\n",
      "Average Rank: 11.0\n",
      "\n",
      "Step: 80000\n",
      "Average Reward: 198.12\n",
      "Average Rank: 10.97\n",
      "\n",
      "Step: 81000\n",
      "Average Reward: 198.07\n",
      "Average Rank: 11.04\n",
      "\n",
      "Step: 82000\n",
      "Average Reward: 198.07\n",
      "Average Rank: 10.98\n",
      "\n",
      "Step: 83000\n",
      "Average Reward: 197.85\n",
      "Average Rank: 11.03\n",
      "\n",
      "Step: 84000\n",
      "Average Reward: 198.2\n",
      "Average Rank: 10.99\n",
      "\n",
      "Step: 85000\n",
      "Average Reward: 197.43\n",
      "Average Rank: 11.04\n",
      "\n",
      "Step: 86000\n",
      "Average Reward: 197.35\n",
      "Average Rank: 11.04\n",
      "\n",
      "Step: 87000\n",
      "Average Reward: 197.35\n",
      "Average Rank: 11.04\n",
      "\n",
      "Step: 88000\n",
      "Average Reward: 197.12\n",
      "Average Rank: 11.05\n",
      "\n",
      "Step: 89000\n",
      "Average Reward: 197.1\n",
      "Average Rank: 11.1\n",
      "\n",
      "Step: 90000\n",
      "Average Reward: 197.1\n",
      "Average Rank: 11.04\n",
      "\n",
      "Step: 91000\n",
      "Average Reward: 196.88\n",
      "Average Rank: 11.02\n",
      "\n",
      "Step: 92000\n",
      "Average Reward: 196.86\n",
      "Average Rank: 11.09\n",
      "\n",
      "Step: 93000\n",
      "Average Reward: 196.86\n",
      "Average Rank: 11.15\n",
      "\n",
      "Step: 94000\n",
      "Average Reward: 197.08\n",
      "Average Rank: 11.09\n",
      "\n",
      "Step: 95000\n",
      "Average Reward: 196.83\n",
      "Average Rank: 11.1\n",
      "\n",
      "Step: 96000\n",
      "Average Reward: 196.83\n",
      "Average Rank: 11.06\n",
      "\n",
      "Step: 97000\n",
      "Average Reward: 196.72\n",
      "Average Rank: 11.05\n",
      "\n",
      "Step: 98000\n",
      "Average Reward: 197.12\n",
      "Average Rank: 11.08\n",
      "\n",
      "Step: 99000\n",
      "Average Reward: 197.18\n",
      "Average Rank: 11.04\n",
      "\n",
      "Step: 100000\n",
      "Average Reward: 197.18\n",
      "Average Rank: 11.06\n",
      "\n",
      "Step: 101000\n",
      "Average Reward: 196.36\n",
      "Average Rank: 11.04\n",
      "\n",
      "Step: 102000\n",
      "Average Reward: 196.01\n",
      "Average Rank: 11.13\n",
      "\n",
      "Step: 103000\n",
      "Average Reward: 196.28\n",
      "Average Rank: 11.15\n",
      "\n",
      "Step: 104000\n",
      "Average Reward: 195.46\n",
      "Average Rank: 11.09\n",
      "\n",
      "Step: 105000\n",
      "Average Reward: 196.46\n",
      "Average Rank: 11.1\n",
      "\n",
      "Step: 106000\n",
      "Average Reward: 196.46\n",
      "Average Rank: 11.07\n",
      "\n",
      "Step: 107000\n",
      "Average Reward: 196.39\n",
      "Average Rank: 11.14\n",
      "\n",
      "Step: 108000\n",
      "Average Reward: 196.71\n",
      "Average Rank: 11.11\n",
      "\n",
      "Step: 109000\n",
      "Average Reward: 195.95\n",
      "Average Rank: 11.06\n",
      "\n",
      "Step: 110000\n",
      "Average Reward: 195.76\n",
      "Average Rank: 11.09\n",
      "\n",
      "Step: 111000\n",
      "Average Reward: 195.73\n",
      "Average Rank: 11.05\n",
      "\n",
      "Step: 112000\n",
      "Average Reward: 195.65\n",
      "Average Rank: 11.08\n",
      "\n",
      "Step: 113000\n",
      "Average Reward: 195.42\n",
      "Average Rank: 11.06\n",
      "\n",
      "Step: 114000\n",
      "Average Reward: 195.74\n",
      "Average Rank: 11.12\n",
      "\n",
      "Step: 115000\n",
      "Average Reward: 195.4\n",
      "Average Rank: 11.05\n",
      "\n",
      "Step: 116000\n",
      "Average Reward: 195.44\n",
      "Average Rank: 11.11\n",
      "\n",
      "Step: 117000\n",
      "Average Reward: 195.05\n",
      "Average Rank: 11.03\n",
      "\n",
      "Step: 118000\n",
      "Average Reward: 193.12\n",
      "Average Rank: 11.1\n",
      "\n",
      "Step: 119000\n",
      "Average Reward: 192.99\n",
      "Average Rank: 11.11\n",
      "\n",
      "Step: 120000\n",
      "Average Reward: 192.84\n",
      "Average Rank: 11.12\n",
      "\n",
      "Step: 121000\n",
      "Average Reward: 193.62\n",
      "Average Rank: 11.15\n",
      "\n",
      "Step: 122000\n",
      "Average Reward: 193.36\n",
      "Average Rank: 11.11\n",
      "\n",
      "Step: 123000\n",
      "Average Reward: 192.65\n",
      "Average Rank: 11.08\n",
      "\n",
      "Step: 124000\n",
      "Average Reward: 192.21\n",
      "Average Rank: 11.04\n",
      "\n",
      "Step: 125000\n",
      "Average Reward: 189.7\n",
      "Average Rank: 11.11\n",
      "\n",
      "Step: 126000\n",
      "Average Reward: 188.07\n",
      "Average Rank: 11.12\n",
      "\n",
      "Step: 127000\n",
      "Average Reward: 187.68\n",
      "Average Rank: 11.15\n",
      "\n",
      "Step: 128000\n",
      "Average Reward: 187.71\n",
      "Average Rank: 11.11\n",
      "\n",
      "Step: 129000\n",
      "Average Reward: 186.55\n",
      "Average Rank: 11.11\n",
      "\n",
      "Step: 130000\n",
      "Average Reward: 184.09\n",
      "Average Rank: 11.09\n",
      "\n",
      "Step: 131000\n",
      "Average Reward: 183.81\n",
      "Average Rank: 11.07\n",
      "\n",
      "Step: 132000\n",
      "Average Reward: 183.99\n",
      "Average Rank: 11.09\n",
      "\n",
      "Step: 133000\n",
      "Average Reward: 183.23\n",
      "Average Rank: 11.06\n",
      "\n",
      "Step: 134000\n",
      "Average Reward: 182.63\n",
      "Average Rank: 11.05\n",
      "\n",
      "Step: 135000\n",
      "Average Reward: 182.85\n",
      "Average Rank: 11.07\n",
      "\n",
      "Step: 136000\n",
      "Average Reward: 184.42\n",
      "Average Rank: 11.1\n",
      "\n",
      "Step: 137000\n",
      "Average Reward: 182.52\n",
      "Average Rank: 11.09\n",
      "\n",
      "Step: 138000\n",
      "Average Reward: 182.07\n",
      "Average Rank: 11.12\n",
      "\n",
      "Step: 139000\n",
      "Average Reward: 180.94\n",
      "Average Rank: 11.08\n",
      "\n",
      "Step: 140000\n",
      "Average Reward: 179.97\n",
      "Average Rank: 11.1\n",
      "\n",
      "Step: 141000\n",
      "Average Reward: 179.75\n",
      "Average Rank: 11.05\n",
      "\n",
      "Step: 142000\n",
      "Average Reward: 180.13\n",
      "Average Rank: 11.08\n",
      "\n",
      "Step: 143000\n",
      "Average Reward: 180.23\n",
      "Average Rank: 11.06\n",
      "\n",
      "Step: 144000\n",
      "Average Reward: 180.05\n",
      "Average Rank: 11.06\n",
      "\n",
      "Step: 145000\n",
      "Average Reward: 178.74\n",
      "Average Rank: 11.07\n",
      "\n",
      "Step: 146000\n",
      "Average Reward: 178.37\n",
      "Average Rank: 11.07\n",
      "\n",
      "Step: 147000\n",
      "Average Reward: 178.46\n",
      "Average Rank: 11.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step: 148000\n",
      "Average Reward: 179.57\n",
      "Average Rank: 11.01\n",
      "\n",
      "Step: 149000\n",
      "Average Reward: 179.45\n",
      "Average Rank: 11.05\n",
      "\n",
      "Step: 150000\n",
      "Average Reward: 178.93\n",
      "Average Rank: 11.09\n",
      "\n",
      "Step: 151000\n",
      "Average Reward: 177.52\n",
      "Average Rank: 11.06\n",
      "\n",
      "Step: 152000\n",
      "Average Reward: 177.59\n",
      "Average Rank: 11.08\n",
      "\n",
      "Step: 153000\n",
      "Average Reward: 176.39\n",
      "Average Rank: 11.13\n",
      "\n",
      "Step: 154000\n",
      "Average Reward: 175.95\n",
      "Average Rank: 11.08\n",
      "\n",
      "Step: 155000\n",
      "Average Reward: 175.72\n",
      "Average Rank: 11.08\n",
      "\n",
      "Step: 156000\n",
      "Average Reward: 175.14\n",
      "Average Rank: 11.07\n",
      "\n",
      "Step: 157000\n",
      "Average Reward: 176.35\n",
      "Average Rank: 11.13\n",
      "\n",
      "Step: 158000\n",
      "Average Reward: 176.8\n",
      "Average Rank: 11.12\n",
      "\n",
      "Step: 159000\n",
      "Average Reward: 177.58\n",
      "Average Rank: 11.11\n",
      "\n",
      "Step: 160000\n",
      "Average Reward: 177.69\n",
      "Average Rank: 11.11\n",
      "\n",
      "Step: 161000\n",
      "Average Reward: 176.92\n",
      "Average Rank: 11.13\n",
      "\n",
      "Step: 162000\n",
      "Average Reward: 178.9\n",
      "Average Rank: 11.13\n",
      "\n",
      "Step: 163000\n",
      "Average Reward: 180.04\n",
      "Average Rank: 11.2\n",
      "\n",
      "Step: 164000\n",
      "Average Reward: 181.11\n",
      "Average Rank: 11.19\n",
      "\n",
      "Step: 165000\n",
      "Average Reward: 180.59\n",
      "Average Rank: 11.1\n",
      "\n",
      "Step: 166000\n",
      "Average Reward: 181.57\n",
      "Average Rank: 11.18\n",
      "\n",
      "Step: 167000\n",
      "Average Reward: 181.18\n",
      "Average Rank: 11.16\n",
      "\n",
      "Step: 168000\n",
      "Average Reward: 178.8\n",
      "Average Rank: 11.17\n",
      "\n",
      "Step: 169000\n",
      "Average Reward: 181.76\n",
      "Average Rank: 11.19\n",
      "\n",
      "Step: 170000\n",
      "Average Reward: 181.08\n",
      "Average Rank: 11.17\n",
      "\n",
      "Step: 171000\n",
      "Average Reward: 182.46\n",
      "Average Rank: 11.17\n",
      "\n",
      "Step: 172000\n",
      "Average Reward: 182.52\n",
      "Average Rank: 11.19\n",
      "\n",
      "Step: 173000\n",
      "Average Reward: 182.92\n",
      "Average Rank: 11.16\n",
      "\n",
      "Step: 174000\n",
      "Average Reward: 185.42\n",
      "Average Rank: 11.15\n",
      "\n",
      "Step: 175000\n",
      "Average Reward: 185.53\n",
      "Average Rank: 11.16\n",
      "\n",
      "Step: 176000\n",
      "Average Reward: 185.74\n",
      "Average Rank: 11.1\n",
      "\n",
      "Step: 177000\n",
      "Average Reward: 184.18\n",
      "Average Rank: 11.09\n",
      "\n",
      "Step: 178000\n",
      "Average Reward: 179.47\n",
      "Average Rank: 11.15\n",
      "\n",
      "Step: 179000\n",
      "Average Reward: 181.92\n",
      "Average Rank: 11.22\n",
      "\n",
      "Step: 180000\n",
      "Average Reward: 182.58\n",
      "Average Rank: 11.23\n",
      "\n",
      "Step: 181000\n",
      "Average Reward: 181.72\n",
      "Average Rank: 11.15\n",
      "\n",
      "Step: 182000\n",
      "Average Reward: 181.58\n",
      "Average Rank: 11.27\n",
      "\n",
      "Step: 183000\n",
      "Average Reward: 182.71\n",
      "Average Rank: 11.2\n",
      "\n",
      "Step: 184000\n",
      "Average Reward: 183.83\n",
      "Average Rank: 11.25\n",
      "\n",
      "Step: 185000\n",
      "Average Reward: 183.83\n",
      "Average Rank: 11.21\n",
      "\n",
      "Step: 186000\n",
      "Average Reward: 186.85\n",
      "Average Rank: 11.31\n",
      "\n",
      "Step: 187000\n",
      "Average Reward: 186.35\n",
      "Average Rank: 11.32\n",
      "\n",
      "Step: 188000\n",
      "Average Reward: 186.15\n",
      "Average Rank: 11.21\n",
      "\n",
      "Step: 189000\n",
      "Average Reward: 186.96\n",
      "Average Rank: 11.24\n",
      "\n",
      "Step: 190000\n",
      "Average Reward: 186.31\n",
      "Average Rank: 11.26\n",
      "\n",
      "Step: 191000\n",
      "Average Reward: 187.51\n",
      "Average Rank: 11.18\n",
      "\n",
      "Step: 192000\n",
      "Average Reward: 187.88\n",
      "Average Rank: 11.27\n",
      "\n",
      "Step: 193000\n",
      "Average Reward: 188.06\n",
      "Average Rank: 11.2\n",
      "\n",
      "Step: 194000\n",
      "Average Reward: 188.25\n",
      "Average Rank: 11.26\n",
      "\n",
      "Step: 195000\n",
      "Average Reward: 188.49\n",
      "Average Rank: 11.21\n",
      "\n",
      "Step: 196000\n",
      "Average Reward: 190.05\n",
      "Average Rank: 11.33\n",
      "\n",
      "Step: 197000\n",
      "Average Reward: 194.23\n",
      "Average Rank: 11.26\n",
      "\n",
      "Step: 198000\n",
      "Average Reward: 195.44\n",
      "Average Rank: 11.28\n",
      "\n",
      "Step: 199000\n",
      "Average Reward: 195.44\n",
      "Average Rank: 11.22\n",
      "\n",
      "Step: 200000\n",
      "Average Reward: 196.4\n",
      "Average Rank: 11.33\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in range(1, MAX_STEPS+1):\n",
    "    \n",
    "    epsilon = np.interp(step, [0, EPSILON_DECAY], [EPSILON_START, EPSILON_END])\n",
    "    \n",
    "    rng = random.random()\n",
    "    if rng <= epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = online_net.act(obs)\n",
    "    \n",
    "    new_obs, reward, done, _ = env.step(action)\n",
    "    transition = (obs, action, reward, done, new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "    \n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        reward_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "    \n",
    "    # watch play\n",
    "    if len(reward_buffer) >= 200:\n",
    "        if np.mean(reward_buffer) >= 195: # once model averages score of 195 (max 200)\n",
    "            while True:\n",
    "                action = online_net.act(obs)\n",
    "                \n",
    "                obs, _, done, _ = env.step(action)\n",
    "                env.render()\n",
    "                if done:\n",
    "                    env.reset()\n",
    "        \n",
    "    # Start Gradient Step   \n",
    "    transitions = random.sample(replay_buffer, BATCH_SIZE)\n",
    "    \n",
    "    observations = np.asarray([t[0] for t in transitions])\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    rewards = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_observations = np.asarray([t[4] for t in transitions])\n",
    "    \n",
    "    observations_t = torch.as_tensor(observations, dtype=torch.float32)\n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_t = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_observations_t = torch.as_tensor(new_observations, dtype=torch.float32)\n",
    "    \n",
    "    # Compute Targets\n",
    "    target_q_values = target_net(new_observations_t)\n",
    "    max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "    \n",
    "    targets = rewards_t + GAMMA * (1 - dones_t) * max_target_q_values\n",
    "    \n",
    "    # Compute Loss\n",
    "    q_values = online_net(observations_t)\n",
    "    action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
    "    loss = loss_function(action_q_values, targets)\n",
    "    \n",
    "    # Gradient Descent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Compute Phi Matrix\n",
    "    phi_matrix = []\n",
    "    phi_matrix = online_net.get_phi(observations_t)\n",
    "    phi_matrix = phi_matrix.cpu().detach().numpy()\n",
    "    rank = np.linalg.matrix_rank(phi_matrix)\n",
    "    all_ranks.append(rank)\n",
    "    last_100_ranks.append(rank)\n",
    "    \n",
    "    # Update Target Network\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "        \n",
    "    # Logging\n",
    "    if step % 1000 == 0:\n",
    "        print()\n",
    "        print(\"Step:\", step)\n",
    "        print(\"Average Reward:\", np.mean(reward_buffer))\n",
    "        print(\"Average Rank:\", np.mean(last_100_ranks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
